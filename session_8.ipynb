{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "# Tree-Based Methods"}, {"cell_type": "markdown", "metadata": {}, "source": "## 8.1 Classification tree"}, {"cell_type": "markdown", "metadata": {}, "source": "This problem involves the `OJ` dataset (`Data/OJ.csv`)  \n**Orange Juice Data**\n\nThe data contains 1070 purchases where the customer either purchased Citrus Hill or Minute Maid Orange Juice. A number of characteristics of the customer and product are recorded.\n\nA data frame with 1070 observations on the following 18 variables:\n\n`Purchase` :A factor with levels CH and MM indicating whether the customer purchased Citrus Hill or Minute Maid Orange Juice  \n`WeekofPurchase`: Week of purchase  \n`StoreID`: Store ID  \n`PriceCH`: Price charged for CH  \n`PriceMM`: Price charged for MM  \n`DiscCH`: Discount offered for CH\n`DiscMM`: Discount offered for MM  \n`SpecialCH`: Indicator of special on CH  \n`SpecialMM`: Indicator of special on MM  \n`LoyalCH`: Customer brand loyalty for CH  \n`SalePriceMM`: Sale price for MM  \n`SalePriceCH`: Sale price for CH  \n`PriceDiff`: Sale price of MM less sale price of CH  \n`Store7`: A factor with levels No and Yes indicating whether the sale is at Store 7  \n`PctDiscMM`: Percentage discount for MM  \n`PctDiscCH`: Percentage discount for CH  \n`ListPriceDiff`: List price of MM less list price of CH  \n`STORE`: Which of 5 possible stores the sale occured at"}, {"cell_type": "markdown", "metadata": {}, "source": "### a)\nCreate a training set containing a random sample of 800 observations, and a test set containing the remaining observations."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "### b)\nLearn a classification tree from the training data using the function `sklearn.tree.DecisionTreeClassifier()`, with\n`Purchase` as the output and the other variables as inputs. Don't forget to handle qulitative variables correctly. To avoid severe overfit, you have to add some constraints to the tree, using, e.g., a maximum depth of 2 (`max_depth=2`)."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "### c)\nUse `sklearn.tree.export_graphviz()`and the `Python` module `graphviz` to visualize the tree and interpret the result. How many terminal nodes does the tree have? Pick one of the terminal nodes, and interpret the information displayed. Type `OJ.info()` to get information about all input variables in the data set."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "### d) \nPredict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "### e)\n\nExplore the different parameters you can pass to the tree command, such as `splitter`, `min_samples_split`, `min_samples_leaf`, `min_impurity_split` etc."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "## 8.2 Random Forest"}, {"cell_type": "markdown", "metadata": {}, "source": "In this exercise we will use the email-spam data that has been presented in a couple of lectures. Go to the URL\nhttp://archive.ics.uci.edu/ml/datasets/Spambase for more information about the data."}, {"cell_type": "markdown", "metadata": {}, "source": "### a)\nLoad the dataset `Data/email.csv`"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "### (b) \nCreate a training set containing a random sample of $75\\%$ of the observations, and a test set containing the remaining observations."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "### (c) \nFit a classification tree with `Class` as output. Compute the test error."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "### (d) \nUse the bagging approach to learn a classifier `sklearn.ensemble.BaggingClassifier()`. What test error do you get?"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "### (e) \nLearn a random forest classifier using the function `sklearn.ensemble.RandomForestClassifier()`. What test error do you get?"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "## 8.3 Bootstrap"}, {"cell_type": "markdown", "metadata": {}, "source": "The bootstrap method has been introduced to reduce the variance in decision trees. However, the bootstrap method is widely applicable in other context as well, for example to estimate the variance in a parameter (cf. bias-variance tradeoff)."}, {"cell_type": "markdown", "metadata": {}, "source": "### (a) \nGenerate $n = 100$ samples $\\{y_i\\}_{i=1}^n$ from $\\mathcal{N}(4, 1^2)$."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "### (b) \nWe want to lean a model $y = \\beta_0 + \\epsilon$ (with $\\mathbb{E}[\\epsilon]=0$) from the data $\\{y_i\\}_{n=1}^n$. Estimate $\\beta_0$ with least squares, i.e. $\\hat{\\beta_0} = \\frac{1}{n}\\sum_{i=1}^n y_i$."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "### (c) \nTo estimate the variance in $\\hat{\\beta}_0$, $\\operatorname{Var}[\\hat{\\beta}_0]$ (a possible \u2018quality measure\u2019 of an estimator), repeat (a)-(b) $1000$ times to get $1000$ estimates of $\\beta_0$, which we call $\\hat{\\beta}_0^1, ..., \\hat{\\beta}_0^{1000}$. Plot a histogram over the estimates $\\hat{\\beta}_0^1, ..., \\hat{\\beta}_0^{1000}$."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "In practice we only have access to $\\{y_i\\}_{n=1}^n$ $(n = 100)$ and cannot estimate $\\hat{\\beta}_0^1, ..., \\hat{\\beta}_0^{1000}$ by generating new data. However, with the bootstrap method we can obtain \u2018new\u2019 data sets by sampling from the original data set $\\{y_i\\}_{n=1}^n$."}, {"cell_type": "markdown", "metadata": {}, "source": "### (d) \nSample $n$ indices $\\{i_1, i_2, ... , i_n\\}$ with replacement from the set $\\{0, 1, ... , n-1\\}$ using the function `numpy.random.choice()`. Note that some indices will appear multiple times and some will not appear at all."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "### (e) \nGenerate a \u2018new\u2019 data set $\\{y_j\\}_{j=1}^n$ based on the indices generated in (d) and estimate $\\hat{\\mu}$ from this data set."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "### (f) \nRepeat (d)-(e) $1000$ times to get $1000$ estimates of $\\hat{\\mu}$, which we call $\\hat{\\mu}_1^*, ..., \\hat{\\mu}_{1000}^*$. Plot a histogram over the estimates $\\hat{\\mu}_1^*, ..., \\hat{\\mu}_{1000}^*$ and compare with the estimate achieved in (c)."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "The distribution of the estimates very similar to the first histogram, but centered\naround $4.04$ which was the mean of this data set. Consequently, we got a fairly accurate estimate of the uncertainity of the estimate without generating new data. For a linear estimator like this one, the uncertainity can be computed analytically (try to do that!), but in many nonlinear cases, this is not tractable or even possible. Then, the bootstrap\nmethod is very powerful."}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.5"}}, "nbformat": 4, "nbformat_minor": 2}