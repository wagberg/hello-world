{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"keep": true, "tags": ["keep"]}, "outputs": [], "source": "import numpy as np\nimport pandas as pd\nimport sklearn.linear_model as skl_lm\nimport matplotlib.pyplot as plt\n\n# Just to get nicer plots\nfrom IPython.display import set_matplotlib_formats\n# set_matplotlib_formats('svg') # Output as svg. Else you can try png\nfrom IPython.core.pylabtools import figsize\nfigsize(10, 6) # Width and hight"}, {"cell_type": "markdown", "metadata": {}, "source": "# 2.1 Problem 1.1 using matrix multiplications\nImplement the linear regression problems from Exercises 1.1(a), (b), (c), (d) and (e) in Python using matrix multiplications.\nA matrix $$\\textbf{X} =  \\begin{bmatrix}\n1 & 2 \\\\\n1 & 3 \\\\ \n\\end{bmatrix}\n$$\ncan be constructed with numpy as `X=np.array([[1, 2], [1, 3]])` (Make sure that `numpy` has been imported. Here it is imported as `np`). The commands for matrix multiplication and transpose in `numpy` are `@` or `np.matmul` and `.T` or `np.transpose()` respectively. A system of linear equations $\\textbf{A}x=\\textbf{b}$ can be solved using `np.linalg.solve(A,b)`.  A $k \\times k$ unit matrix can be constructed with `np.eye(k)`.\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## (a)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "## (b)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "## (c)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "## (d)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "## (e)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "# 2.2 Problem 1.1 using the linear_model.LinearRegression() command\nImplement the linear regression problem from Exercises 1.1(b) and (c) using the command `LinearRegression()` from `sklearn.linear_model`. "}, {"cell_type": "markdown", "metadata": {}, "source": "## (b)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "## (c)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "# 2.3 The Auto data set"}, {"cell_type": "markdown", "metadata": {}, "source": "## (a)\nLoad the dataset `'Data/Auto.csv'`. Familiarize yourself with the dataset using `Auto.info()`. The dataset:  \n\n**Description**:  Gas mileage, horsepower, and other information for 392 vehicles.  \n**Format**: A data frame with 392 observations on the following 9 variables.  \n\n- `mpg`: miles per gallon  \n- `cylinders`: Number of cylinders between 4 and 8\n- `displacement`: Engine displacement (cu. inches)\n- `horsepower`: Engine horsepower\n- `weight`: Vehicle weight (lbs.)\n- `acceleration`: Time to accelerate from 0 to 60 mph (sec.)\n- `year`: Model year (modulo 100)\n- `origin`: Origin of car (1. American, 2. European, 3. Japanese)\n- `name`: Vehicle name  \n*The orginal data contained 408 observations but 16 observations with missing values were removed.*\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "## (b)\nDivide the data set randomly into two approximately equally sized subsets, `train` and `test` by generating the random indices using `np.random.choice()`.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "## (c)\nPerform linear regression with `mpg` as the output and all other variables except name as input. How well (in terms of root-mean-square-error) does the model perform on test data and training data, respectively?\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "## (d)\nNow, consider the input variable `origin`. What do the different numbers represent? By running `Auto.origin.sample(30)` we see the 30 samples of the variable and that the input variables is quantitative. Does it really makes sense to treat it as a quantitative input? Use `np.get_dummies()` to split it into dummy variables and do the linear regression again.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "## (e)\nTry obtain a better RMSE on test data by removing some inputs (explore what happens if you remove, e.g, `year`, `weight` and `acceleration`)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "## (f)\nTry to obtain a better RMSE on test data by adding some transformations of inputs, such as \n$log(x)$, $\\sqrt{x}$, $x_1x_2$ etc.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "# 2.4 Nonlinear transformations of input variables"}, {"cell_type": "markdown", "metadata": {}, "source": "Start by running the following code to generate your training data\n```python\nnp.random.seed(1)\nx_train = np.random.uniform(0, 10, 100)\ny_train = .4 - .6 * x_train + 3. * np.sin(x_train - 1.2) + np.random.normal(0, 0.1, 100)\n```\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "## (a) \nPlot the training output y_train versus the training input x_train.  "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "## (b) \nLearn a model on the form \n$$y= a + bx + c sin(x + \\phi) + \\epsilon,  \\qquad  \\epsilon \\sim \\mathcal{N}(0, 0,1^2) \\qquad (2.1)$$\n\nwhere all parameters $a$, $b$, $c$ and $\\phi$ are to be learned from the training data `x_train` and `y_train`. Refrain from using the` linear_model()` command, but implement the normal equations yourself as in problem 2.1. Hint: Even though (2.1) is not a linear regression model, you can use the fact that $c sin(x + \\phi) = c cos(\\phi) sin(x) + c sin(\\phi) cos(x)$ to transform it into one.  \n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "## (c) \nConstruct 100 test inputs `x_test` in the span from 0 to 10 by using the `np.linspace()` function. Predict the outputs corresponding to these inputs and plot them together with the training data."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "## (d) \nDo a least squares fit by instead using the `linear_model()` function in `Python`. Check that you get the same estimates as in (b)."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "# 2.5  Regularization"}, {"cell_type": "markdown", "metadata": {}, "source": "In this exercise we will apply Ridge regression and Lasso for fitting a polynomial to a scalar data set. We will have a setting where we first generate synthetic training data from \n\n$$y = x^3 + 2x^2 + 6 + \\epsilon, \\qquad (2.2)$$ and later try to learn model for the data.   "}, {"cell_type": "markdown", "metadata": {}, "source": "## (a) \nWrite a function that implements the polynomial (2.2), i.e., takes $x$ as argument and returns $x^3 + 2x^2 + 6$. "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "## (b) \nUse `np.random.seed()` to set the random seed. Use the function `np.linspace()` to construct a vector `x` with `n = 12` elements equally spaced from $-2.3$ to $1$. Then use your function from (a) to construct a vector $\\textbf{y} = [y_1, ..., y_n]^T$ with 12 elements, where $y = x^3 + 2x^2 + 6 + \\epsilon$, with $\\epsilon \\sim  \\mathcal{N(0, 1^2)}$. This is our training data."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "## (c) \nPlot the training data $\\mathcal{T} = \\{x_i, y_i\\}_{i=1}^{12}$ together with the \"true\" function."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "## (d)\nFit a straight line to the data with $y$ as output and $x$ as input and plot the predicted output $\\hat{y}_{\\star}$ for densely spaced $x_{\\star}$ values between $-2.3$ and $1$. Plot these predictions in the same plot window."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "## (e) \nFit a 11th degree polynomial to the data with linear regression. Plot the corresponding predictions."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": "## (f) \n\nUse the fucntion `sklearn.linear_model.Ridge` and `sklearn.linear_model.Lasso` to fit a 11th degree polynomial. Also inspect the estimated coefficients. Try different values of penalty term $\\alpha$. What do you observe?\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.9"}}, "nbformat": 4, "nbformat_minor": 2}